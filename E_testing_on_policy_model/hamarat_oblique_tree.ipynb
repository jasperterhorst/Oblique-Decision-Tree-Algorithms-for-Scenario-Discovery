{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-28T12:49:05.261308Z",
     "start_time": "2025-05-28T12:49:02.046667Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from src.config.paths import HAMARAT_DATA_DIR\n",
    "from C_oblique_decision_tree_benchmark.evaluation.benchmark_runner import DepthSweepRunner\n",
    "\n",
    "# Load the experiments (inputs)\n",
    "experiments = pd.read_csv(HAMARAT_DATA_DIR / \"experiments.csv\")\n",
    "\n",
    "# Load the outcomes (outputs)\n",
    "fraction_renewables = pd.read_csv(HAMARAT_DATA_DIR / \"fraction renewables.csv\", header=None)\n",
    "carbon_emissions_reduction = pd.read_csv(HAMARAT_DATA_DIR / \"carbon emissions reduction fraction.csv\", header=None)\n",
    "\n",
    "# Optional: Load time vector (if provided separately)\n",
    "time = pd.read_csv(HAMARAT_DATA_DIR / \"TIME.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments: (5000, 48)\n",
      "Fraction Renewables: (5000, 641)\n",
      "Carbon Emissions Reduction: (5000, 641)\n"
     ]
    }
   ],
   "source": [
    "print(\"Experiments:\", experiments.shape)\n",
    "print(\"Fraction Renewables:\", fraction_renewables.shape)\n",
    "print(\"Carbon Emissions Reduction:\", carbon_emissions_reduction.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-28T12:49:14.030342Z",
     "start_time": "2025-05-28T12:49:14.025973Z"
    }
   },
   "id": "ad05d3abb9594381",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extract the final value (i.e., for the year 2050) of the renewable fraction for each simulation run.\n",
    "# Each row corresponds to a scenario, and each column is a quarterly timestep from 2010 to 2050.\n",
    "# We use the final column to classify whether the energy transition was successful.\n",
    "final_renewable_share = fraction_renewables.iloc[:, -1]\n",
    "\n",
    "# Define a binary classification label based on the policy outcome:\n",
    "# If the renewable share in 2050 is less than 60%, we consider the outcome undesirable (label = 1),\n",
    "# otherwise the outcome is acceptable (label = 0).\n",
    "label = (final_renewable_share < 0.60).astype(int)\n",
    "\n",
    "# Add this binary classification label to the original experiments DataFrame,\n",
    "# so that each row (scenario) is now associated with a success/failure outcome.\n",
    "experiments[\"label\"] = label\n",
    "\n",
    "# Define which columns to exclude from the feature matrix:\n",
    "# - \"label\" is the target and should not be part of the input features.\n",
    "# - \"model\", \"policy\", and \"year\" are metadata fields, not true decision variables.\n",
    "columns_to_drop = [\"label\", \"model\", \"policy\", \"year\"]\n",
    "\n",
    "# Create the input features matrix (X) by dropping non-feature columns.\n",
    "# This DataFrame will be passed to the decision tree model.\n",
    "X_df = experiments.drop(columns=columns_to_drop)\n",
    "\n",
    "# Extract the label column as a NumPy array (required by most ML algorithms).\n",
    "# This will be used as the classification target (y).\n",
    "y_array = experiments[\"label\"].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-28T12:49:15.103366Z",
     "start_time": "2025-05-28T12:49:15.096085Z"
    }
   },
   "id": "d69e53bb685b4248",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Wrap the cleaned dataset into a dictionary format expected by DepthSweepRunner.\n",
    "# The key is a string identifier for the dataset, and the value is a tuple (X, y),\n",
    "# where X is a DataFrame of input features and y is a NumPy array of binary labels.\n",
    "# This structure supports multiple datasets if needed.\n",
    "datasets = {\n",
    "    \"hamarat_energy_transition\": (X_df, y_array)\n",
    "}\n",
    "\n",
    "# Specify the algorithm you want to benchmark.\n",
    "# Options include: \"hhcart_a\", \"hhcart_d\", \"cart\", \"randcart\", \"ridge_cart\", \"oc1\", \"co2\", \"wodt\"\n",
    "# Here, we select HHCART with diagonal reflection strategy (\"hhcart_d\").\n",
    "algorithm = \"hhcart_d\"\n",
    "\n",
    "# Build a registry that includes only the selected algorithm.\n",
    "# The registry maps algorithm names to functions that instantiate decision tree classifiers.\n",
    "# Although build_registry() returns all available models, we extract only the one we need.\n",
    "registry = {\n",
    "    algorithm: DepthSweepRunner.build_registry()[algorithm]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-28T12:49:15.246975Z",
     "start_time": "2025-05-28T12:49:15.243739Z"
    }
   },
   "id": "a28be5525ba3554b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Depth Sweeping: 100%|██████████| 5/5 [34:00<00:00, 408.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved DataFrame to: C:\\Users\\jaspe\\OneDrive\\Desktop\\Oblique-Decision-Tree-Algorithms-for-Scenario-Discovery\\_data\\depth_sweep_single_run_results\\hhcart_d_hamarat.csv\n",
      "[OK] Saved trees_dict to: C:\\Users\\jaspe\\OneDrive\\Desktop\\Oblique-Decision-Tree-Algorithms-for-Scenario-Discovery\\_data\\depth_sweep_single_run_results\\hhcart_d_hamarat.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the DepthSweepRunner, which manages the benchmarking process.\n",
    "# - datasets: dictionary containing the dataset(s) to run (here, just one: \"hamarat_energy_transition\")\n",
    "# - max_depth: the maximum tree depth to explore during the sweep.\n",
    "#   This means trees will be trained at depths 0 through 4 (inclusive).\n",
    "runner = DepthSweepRunner(datasets=datasets, max_depth=4)\n",
    "\n",
    "# Run the benchmarking sweep:\n",
    "# - registry: specifies which algorithm(s) to benchmark — here, just \"hhcart_d\"\n",
    "# - auto_export: if True, saves the results DataFrame to CSV automatically\n",
    "# - filename: the name of the CSV file where benchmarking results will be saved\n",
    "# - tree_dict_filename: the name of the .pkl file where all trained trees will be saved\n",
    "# The runner returns:\n",
    "# - results_df: a DataFrame containing metrics for each (algorithm, depth) combination\n",
    "# - tree_dict: a dictionary holding the trained trees for later inspection or visualisation\n",
    "results_df, tree_dict = runner.run(\n",
    "    registry=registry,\n",
    "    auto_export=True,\n",
    "    filename=f\"{algorithm}_hamarat.csv\",\n",
    "    tree_dict_filename=f\"{algorithm}_hamarat.pkl\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-19T10:50:55.561958Z",
     "start_time": "2025-05-19T10:16:55.043966Z"
    }
   },
   "id": "5c41a8ef408321ce",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ef122f2ff76a1c9b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
